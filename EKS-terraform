AWS EKS(Elastic Kubernetes Service) is a managed service we can use to run kubernetes on AWS without need of install, operate and to maintain your own kubernetes control Plane/Nodes.
It scales, manages and deploys containerized applications which can run on Amazon public cloud..
Amazon EKS runs across multiple avilability zones(AZ). AWS EKS provide highly avilable and secure clusters and automates key tasks such as patching, node provisioning and updates.

AWS EKS are composed with:
Control Plane: Composed of 3 master nodes, each running in a single zone or different avilablility zones(for high avilability).
Worker Nodes: Runs on amazon EC2 instance located in a VPC, we can control and configure the VPC allocated for worker nodes.

Provison         ----->      Deploy    ------>    Connect   ------>  Run Kubernetes 
an EKS Cluster               Compute              to EKS             Applications

Two types of deployment options:-
1) Deploy one cluster for each environment/application.
2) Define IAM security polocies and kubernetes namespaces to deploy one cluster for multiple applications/environments.

EKS provides support of Amazon VPC network polocies for restricting traffic between control-plane and cluster.
Only authorised cluster and accounts, defined by role-based access control(RABC), can view or communicate with control pane.


To deploy an EKS cluster with terraform we need:
> AWS account
> IAM credentilas and access.
> VPC configured for EKS.
Code text editor(VS Code).


Create a vars.tf file and apply the AWS access key and secret key of IAM user with sufficient previleges to make a connection between VS code and AWS.

vars.tf 
variable "access_key" {
  default = "A---------------X"
}
variable "secret_key" {
    default = "y-------------------------h"
}

Create a main.tf file and name the provider and region as per the requirement.
main.tf

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 3.20.0"
    }

    random = {
      source  = "hashicorp/random"
      version = "3.1.0"
    }

    local = {
      source  = "hashicorp/local"
      version = "2.1.0"
    }

    null = {
      source  = "hashicorp/null"
      version = "3.1.0"
    }

    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.0.1"
    }
  }

  required_version = ">= 0.14"
}

Create a vpc.tf file, I am using AWS VPC(Virtual Private Cloud) module for the VPC creation.
Provide the name of the VPC, CIDR range 10.0.0.0/16 and region.
AWS VPC has 3 Private and Public subnets.
I have also enabled NAT Gateway and DNS HOSTNAME in our VPC.
And, data aws_avilability_zones and azs provides the list of avilability zone for the provided region.

vpc.tf

variable "region" {
    default = "ap-south-1"
}
data "aws_availability_zones" "available" {}
locals {
    cluster_name = "Bhargav-EKS-Cluster"
}
module vpc {
    source = "terraform-aws-modules/vpc/aws"

    name = "Bhargav-EKS-VPC"
    cidr = "10.0.0.0/16"

    azs = data.aws_availability_zones.available.names
    private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
    public_subnets =  ["10.0.4.0/24", "10.0.5.0/24", "10.0.6.0/24"]

    enable_nat_gateway = true
    single_nat_gateway = true

  enable_dns_hostnames= true
tags = {
    "Name" = "Bhargav-EKS-VPC"
}
public_subnet_tags = {
    "Name" = "EKS-Public-Subnet"
}
private_subnet_tags = {
    "Name" = "EKS-Private-Subnet"
}
}

Create sg.tf file for AWS security group and I have created 2 security groups for 2 worker Nodes Groups.
Port 22 is open for SSH connections and I have restricted access for 10.0.0.0/8 CIDR.

sg.tf

resource "aws_security_group" "worker_group_one" {
    name_prefix = "worker_group_one"
    vpc_id = module.vpc.vpc_id
ingress {
        from_port = 22
        to_port = 22
        protocol = "tcp"
cidr_blocks = [
            "10.0.0.0/8"
        ]
    }
}
resource "aws_security_group" "worker_group_two" {
    name_prefix = "worker_group_two"
    vpc_id = module.vpc.vpc_id

    ingress {
        from_port = 22
        to_port = 22
        protocol = "tcp"
cidr_blocks = [
            "10.0.0.0/8"
        ]
    }
}
resource "aws_security_group" "all_worker_management" {
    name_prefix = "all_worker_management"
    vpc_id = module.vpc.vpc_id
ingress {
        from_port = 22
        to_port = 22
        protocol = "tcp"
cidr_blocks = [
            "10.0.0.0/8"
        ]
    }
}

Create eks.tf file for EKS cluster and I am using AWS EKS module for the cluster creation.
The below configuration creates 2 worker groups (1 & 2) with the desired capacity of 3 instances of type t2.medium.

eks.tf

module "eks"{
    source = "terraform-aws-modules/eks/aws"
    version = "17.18.0"
    cluster_name = local.cluster_name
    cluster_version = "1.23"
    subnets = module.vpc.private_subnets
tags = {
        Name = "Bhargav-EKS-Cluster"
    }
vpc_id = module.vpc.vpc_id
    workers_group_defaults = {
        root_volume_type = "gp3"
    }
worker_groups = [
        {
            name = "Worker-Group-1"
            instance_type = "t2.medium"
            asg_desired_capacity = 2
            additional_security_group_ids = [aws_security_group.worker_group_one.id]
        },
        {
            name = "Worker-Group-2"
            instance_type = "t2.medium"
            asg_desired_capacity = 1
            additional_security_group_ids = [aws_security_group.worker_group_two.id]
        },
    ]
}

data "aws_eks_cluster" "cluster" {
    name = module.eks.cluster_id
}
data "aws_eks_cluster_auth" "cluster" {
    name = module.eks.cluster_id
}

Create kubernetes.tf file for the kubernetes provider and ensure to use recently created EKS cluster as the hiost and use token for the authentication and cluster_ca_certificate for CA certificate.

kubernetes.tf

provider "kubernetes" {

    host = data.aws_eks_cluster.cluster.endpoint
    token = data.aws_eks_cluster_auth.cluster.token
    cluster_ca_certificate = base64encode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
}


output.tf

output "cluster_id" {
    value = module.eks.cluster_id
}
output "cluster_endpoint" {
    value = module.eks.cluster_endpoint
}



